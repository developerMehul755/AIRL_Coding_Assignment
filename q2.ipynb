{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Cloning the githbub repo of GroundingDino and installing the required dependencies."
      ],
      "metadata": {
        "id": "wcurLEDJOCxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHK70yj9f4LA"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd GroundingDINO/\n",
        "!pip install -q -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We now download the GroundingDino pre-trained model's weights"
      ],
      "metadata": {
        "id": "q4QhB0m5OVE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir weights\n",
        "%cd weights\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "%cd .."
      ],
      "metadata": {
        "id": "NxhKhO9xf6B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cloning the github repo of SAM2 model and installing the required dependencies"
      ],
      "metadata": {
        "id": "_hecAwYfOn8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd segment-anything-2\n",
        "!pip install -q -e ."
      ],
      "metadata": {
        "id": "M7ckdXSWgCZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We also need to download the pre-trained SAM2 model checkpoints."
      ],
      "metadata": {
        "id": "8tZnTS3wO4rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd segment-anything-2\n",
        "!mkdir -p checkpoints/\n",
        "!wget -P checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
        "%cd .."
      ],
      "metadata": {
        "id": "A36pW3nWgGWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "wNjbC7crgbrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing some important libraries"
      ],
      "metadata": {
        "id": "Kby6ODK7PCrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/GroundingDINO\n",
        "from groundingdino.util.inference import load_model, load_image, predict, annotate"
      ],
      "metadata": {
        "id": "JKQ2jInygsFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor"
      ],
      "metadata": {
        "id": "oajlunDKgtjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "from torchvision.ops import box_convert\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "UcgrpqTfgwRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We first explore GroundingDino method that takes image and text prompt and output a list of bounding boxes that matches the prompt."
      ],
      "metadata": {
        "id": "fVe4xXwUPLHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading pre-trained model\n",
        "model = load_model(\"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"GroundingDINO/weights/groundingdino_swint_ogc.pth\")"
      ],
      "metadata": {
        "id": "YTxaF8q_hEHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requesting image from cocodataset"
      ],
      "metadata": {
        "id": "2UOCp4atP9XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "urllib.request.urlretrieve(url, \"cats.jpg\")"
      ],
      "metadata": {
        "id": "KjU5kRUykv31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading image and transforming it into resized tensors suitable for box prediction"
      ],
      "metadata": {
        "id": "1jcak_OKQfbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_source = Image.open(\"cats.jpg\").convert(\"RGB\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "image = transform(image_source)\n",
        "image_source = np.array(image_source)"
      ],
      "metadata": {
        "id": "UHahGy12l8gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_PROMPT = \"cats. remote. \"\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25"
      ],
      "metadata": {
        "id": "FWPUqjgdhHxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using predict() function that we imported earlier to predict the bounding boxes based on input prompt"
      ],
      "metadata": {
        "id": "I-CZP-VCQ1P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boxes, logits, phrases = predict(\n",
        "    model=model,\n",
        "    image=image,\n",
        "    caption=TEXT_PROMPT,\n",
        "    box_threshold=BOX_TRESHOLD,\n",
        "    text_threshold=TEXT_TRESHOLD,\n",
        "    device='cpu'\n",
        ")"
      ],
      "metadata": {
        "id": "_akttjbWhsRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can use the annotate() function to create an image with the output bounding boxes and their respective category names and confidence scores."
      ],
      "metadata": {
        "id": "JhHSoz16RETy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)"
      ],
      "metadata": {
        "id": "dAZ7tusxhvCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WtM55SYZhxzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before passing these boxes to our SAM2 model, we need to do some modifications to the format of the bounding boxes"
      ],
      "metadata": {
        "id": "M8gGDefNRqzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h, w, _ = image_source.shape\n",
        "boxes_unnorm = boxes * torch.Tensor([w, h, w, h]) #unnormalizing the range of boxes\n",
        "boxes_xyxy = box_convert(boxes=boxes_unnorm, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy() #converting to xyxy format"
      ],
      "metadata": {
        "id": "3HqfD9joh0Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2"
      ],
      "metadata": {
        "id": "44rdp77ch9_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the pre-trained SAM2 model. Here we choose hiera_large model, so we specify its corresponding checkpoints that we downloaded earlier.We then create a SAM2 model and a predictor to predict the segments"
      ],
      "metadata": {
        "id": "kbR4XGc-SVY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sam2_checkpoint = \"checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device='cpu')\n",
        "\n",
        "predictor = SAM2ImagePredictor(sam2_model)"
      ],
      "metadata": {
        "id": "HywLREDrh_1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define some helper function responsible for plotting the result"
      ],
      "metadata": {
        "id": "QthtPxIGSpFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(3)\n",
        "\n",
        "def show_mask(mask, ax, random_color = False, borders = True):\n",
        "  if random_color:\n",
        "    color = np.concatenate([np.random.random(3), np.array([0.6])], axis= 0)\n",
        "  else:\n",
        "    color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "  h, w = mask.shape[-2:]\n",
        "  mask = mask.astype(np.uint8)\n",
        "  mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "  if borders:\n",
        "        import cv2\n",
        "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "        # Try to smooth contours\n",
        "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
        "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
        "  ax.imshow(mask_image)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
      ],
      "metadata": {
        "id": "GBNn8JsZiDZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('/content/cats.jpg')\n",
        "image = np.array(image.convert(\"RGB\"))\n",
        "predictor.set_image(image)"
      ],
      "metadata": {
        "id": "FwQIPYm3iX_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, _ = predictor.predict(\n",
        "    point_coords=None,\n",
        "    point_labels=None,\n",
        "    box=boxes_xyxy,\n",
        "    multimask_output=False,\n",
        ")"
      ],
      "metadata": {
        "id": "UWIN5upFiekX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "for mask in masks:\n",
        "    show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
        "for box in boxes_xyxy:\n",
        "    show_box(box, plt.gca())\n",
        "plt.axis('off')\n",
        "plt.savefig(\"desk-segment.jpg\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7EIqnXHZihhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting GroundingDINO And SAM2 Together"
      ],
      "metadata": {
        "id": "GlD0wXOBTu5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"../GroundingDINO/weights/groundingdino_swint_ogc.pth\")\n",
        "sam2_model = build_sam2(\"configs/sam2.1/sam2.1_hiera_l.yaml\", \"checkpoints/sam2.1_hiera_large.pt\", device='cpu')"
      ],
      "metadata": {
        "id": "MRbq1-sTjTuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definig a single function which takes input a image path, text_prompt and show you the segmented region in the image on the basis of prompt."
      ],
      "metadata": {
        "id": "Asb4hqScUB4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_with_prompt(image_path, text_prompt, model = model, predictor = predictor, box_threshold = 0.35, text_threshold = 0.25):\n",
        "  image_source, image = load_image(image_path)\n",
        "\n",
        "  boxes, logits, phrases = predict(\n",
        "      model = model,\n",
        "      image = image,\n",
        "      caption = text_prompt,\n",
        "      box_threshold = box_threshold,\n",
        "      text_threshold = text_threshold,\n",
        "      device = 'cpu'\n",
        "  )\n",
        "\n",
        "  annoted_frame = annotate(image_source = image_source, boxes = boxes, logits = logits, phrases = phrases)\n",
        "\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.imshow(cv2.cvtColor(annoted_frame, cv2.COLOR_BGR2RGB))\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  h, w, _ = image_source.shape\n",
        "  boxes_unnorm = boxes * torch.Tensor([w, h, w, h])\n",
        "  boxes_xyxy = box_convert(boxes=boxes_unnorm, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
        "\n",
        "  image = Image.open(image_path)\n",
        "  image = np.array(image.convert(\"RGB\"))\n",
        "  predictor.set_image(image)\n",
        "\n",
        "  masks, scores, _ = predictor.predict(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        box=boxes_xyxy,\n",
        "        multimask_output=False,\n",
        "  )\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.imshow(image)\n",
        "  for mask in masks:\n",
        "      if len(mask.shape) > 2:\n",
        "            show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
        "      else:\n",
        "            show_mask(mask, plt.gca(), random_color=True)\n",
        "  for box in boxes_xyxy:\n",
        "      show_box(box, plt.gca())\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  return masks"
      ],
      "metadata": {
        "id": "pc55PenmoHws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "awv90kn141CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-based Video Segmentation task"
      ],
      "metadata": {
        "id": "MwUVkWRKUV_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and store video file in the current directory"
      ],
      "metadata": {
        "id": "gX-7P1fYUceV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "video_url = \"https://www.pexels.com/download/video/3987730/\"\n",
        "output_filename = \"Horse_running.mp4\"\n",
        "\n",
        "response = requests.get(video_url, stream=True, allow_redirects=True)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open(output_filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=1024*1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "    print(f\"✅ Video downloaded successfully and saved as '{output_filename}'\")\n",
        "else:\n",
        "    print(f\"❌ Failed to download video. Status code: {response.status_code}\")\n",
        "    print(\"This can sometimes happen with temporary links. Please try again or check the URL.\")"
      ],
      "metadata": {
        "id": "YXKPk2OC3jWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"segmented_frames\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "-vogEQXSGnhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_dir = \"frames\"\n",
        "os.makedirs(frame_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "Rc5I7_z53Lkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracts all frames from a video file and saves them as individual image files in a specified directory."
      ],
      "metadata": {
        "id": "-HKbpUC0VU8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(output_filename)\n",
        "frame_count = 0\n",
        "while True:\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    break\n",
        "  frame_filename = os.path.join(frame_dir, f\"{frame_count:05d}.jpeg\")\n",
        "  cv2.imwrite(frame_filename, frame)\n",
        "  frame_count += 1\n",
        "cap.release()"
      ],
      "metadata": {
        "id": "kAQEGm944krz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading another SAM2 model and GroundingDino model"
      ],
      "metadata": {
        "id": "mBojoYcrYL7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2\n",
        "sam2_model_2 = build_sam2(\"configs/sam2.1/sam2.1_hiera_l.yaml\", \"checkpoints/sam2.1_hiera_large.pt\", device='cpu')\n",
        "model_2 = load_model(\"../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"../GroundingDINO/weights/groundingdino_swint_ogc.pth\")\n",
        "predictor_2 = SAM2ImagePredictor(sam2_model_2)"
      ],
      "metadata": {
        "id": "OuM3pRXQ5kCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_with_prompt_2(image_path, text_prompt, model = model_2, predictor = predictor_2, box_threshold = 0.35, text_threshold = 0.25):\n",
        "  image_source, image = load_image(image_path)\n",
        "\n",
        "  boxes, logits, phrases = predict(\n",
        "      model = model,\n",
        "      image = image,\n",
        "      caption = text_prompt,\n",
        "      box_threshold = box_threshold,\n",
        "      text_threshold = text_threshold,\n",
        "      device = 'cpu'\n",
        "  )\n",
        "  h, w, _ = image_source.shape\n",
        "  boxes_unnorm = boxes * torch.Tensor([w, h, w, h])\n",
        "  boxes_xyxy = box_convert(boxes=boxes_unnorm, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
        "\n",
        "  image = Image.open(image_path)\n",
        "  image = np.array(image.convert(\"RGB\"))\n",
        "  predictor.set_image(image)\n",
        "\n",
        "  masks, scores, _ = predictor.predict(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        box=boxes_xyxy,\n",
        "        multimask_output=False,\n",
        "  )\n",
        "\n",
        "  return masks"
      ],
      "metadata": {
        "id": "2toTBVq_-EJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segments the first frame of the video using a text prompt and SAM 2, returning the mask for the specified object to use as a reference for mask propagation."
      ],
      "metadata": {
        "id": "RNT0pTjCYkBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_frame_path = os.path.join('/content/frames', \"00000.jpeg\")\n",
        "prompt = \"horse\"\n",
        "masks = segment_with_prompt_2(first_frame_path, prompt, model = model_2, predictor = predictor)\n",
        "base_mask = masks[0]"
      ],
      "metadata": {
        "id": "7v_kZLft9bqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_mask.shape"
      ],
      "metadata": {
        "id": "etrDf3vSEG9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propagates the first frame’s object mask to subsequent frames, overlays it in red, and saves each frame as an image."
      ],
      "metadata": {
        "id": "SB62qKi8YuFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(200):\n",
        "    frame_path = os.path.join(\"/content/frames\", f\"{i:05d}.jpeg\")\n",
        "    image = Image.open(frame_path).convert(\"RGB\")\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    y_indices, x_indices = np.where(base_mask)[1:]\n",
        "    if len(x_indices) == 0 or len(y_indices) == 0:\n",
        "        continue\n",
        "    x_min, x_max = x_indices.min(), x_indices.max()\n",
        "    y_min, y_max = y_indices.min(), y_indices.max()\n",
        "    box_xyxy = np.array([[x_min, y_min, x_max, y_max]])\n",
        "\n",
        "    predictor.set_image(image_np)\n",
        "    masks, _, _ = predictor.predict(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        box=box_xyxy,\n",
        "        multimask_output=False,\n",
        "    )\n",
        "\n",
        "    mask_rgb = np.zeros_like(image_np)\n",
        "    mask_rgb[:, :, 0] = masks[0] * 255  # Red overlay\n",
        "    alpha = 0.5\n",
        "    overlayed = cv2.addWeighted(image_np, 1, mask_rgb, alpha, 0)\n",
        "\n",
        "    out_path = os.path.join(\"/content/segmented_frames\", f\"{i:05d}.png\")\n",
        "    cv2.imwrite(out_path, cv2.cvtColor(overlayed, cv2.COLOR_RGB2BGR))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "st2uwyaq9xYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combines all segmented frame images into a single MP4 video at 30 fps."
      ],
      "metadata": {
        "id": "H1dPvM0CjkBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame_dir = \"/content/segmented_frames\"\n",
        "output_video = \"/content/segmented_video.mp4\"\n",
        "\n",
        "frame_files = sorted([f for f in os.listdir(frame_dir) if f.endswith(\".png\")])\n",
        "\n",
        "# Read the first frame to get dimensions\n",
        "first_frame = cv2.imread(os.path.join(frame_dir, frame_files[0]))\n",
        "height, width, layers = first_frame.shape\n",
        "\n",
        "# Define video writer (30 fps, adjust if needed)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video, fourcc, 30, (width, height))\n",
        "\n",
        "# Write all frames into video\n",
        "for file in frame_files:\n",
        "    frame = cv2.imread(os.path.join(frame_dir, file))\n",
        "    out.write(frame)\n",
        "\n",
        "out.release()\n",
        "print(\"Segmented video saved as\", output_video)"
      ],
      "metadata": {
        "id": "--HujNmoEAlF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}