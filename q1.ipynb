{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Required Libraries"
      ],
      "metadata": {
        "id": "rQLO8xMLhZ4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8zBxOm1FVnF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "SyKVgdSeFXjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the seed"
      ],
      "metadata": {
        "id": "Z8ZVSsJ7heyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "GDUJ-JyzFnFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the model and training parameters"
      ],
      "metadata": {
        "id": "cFQ_4Nh-hive"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 3e-4\n",
        "PATCH_SIZE = 4\n",
        "NUM_CLASSES = 10\n",
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "EMBED_DIM = 256\n",
        "NUM_HEAD = 8\n",
        "DEPTH = 6\n",
        "MLP_DIM = 512\n",
        "DROP_RATE = 0.1"
      ],
      "metadata": {
        "id": "4GmLn6HpGYfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Transformations applied to test and train data"
      ],
      "metadata": {
        "id": "yAd-bdkUhppb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                     std=[0.2023, 0.1994, 0.2010])\n",
        "])"
      ],
      "metadata": {
        "id": "8JQpwJmCHCFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset and defining the train and test loader"
      ],
      "metadata": {
        "id": "4yM72zKDhw9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(transformation):\n",
        "  train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transformation)\n",
        "  test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transformation)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "  return train_dataset, test_dataset, train_loader, test_loader"
      ],
      "metadata": {
        "id": "QTaHf8_w7veS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset, train_loader, test_loader = load_data(transform)"
      ],
      "metadata": {
        "id": "UU0BjYcR8K8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining PatchEmbedding module that splits an image into patches using a convolution layer, flattens them into embeddings, adds a learnable class token and positional encodings, and outputs the sequence for a Vision Transformer."
      ],
      "metadata": {
        "id": "UdyV8Z6Xh42V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels, embed_dim, stride=None):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.stride = stride if stride is not None else patch_size\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=self.stride\n",
        "        )\n",
        "\n",
        "        # Compute number of patches per dimension\n",
        "        num_patches_per_dim = ((img_size - patch_size) // self.stride) + 1\n",
        "        num_patches = num_patches_per_dim ** 2\n",
        "\n",
        "        # Class token + positional embedding\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        B = x.size(0)\n",
        "        x = self.proj(x)                        # [B, embed_dim, H', W']\n",
        "        x = x.flatten(2).transpose(1, 2)        # [B, num_patches, embed_dim]\n",
        "        cls_token = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_token, x], dim=1)    # [B, 1+num_patches, embed_dim]\n",
        "        x = x + self.pos_embed\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "eU1te4uq9D4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_features,\n",
        "               hidden_features,\n",
        "               drop_rate):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features=in_features,\n",
        "                         out_features=hidden_features)\n",
        "    self.drop = nn.Dropout(p=drop_rate)\n",
        "    self.fc2 = nn.Linear(in_features=hidden_features,\n",
        "                         out_features=in_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.drop(F.gelu(self.fc1(x)))\n",
        "    x = self.drop(self.fc2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "kKQ_Gq3vEdkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing a Multi-Head Self-Attention layer, which projects input embeddings into queries, keys, and values, computes scaled dot-product attention across multiple heads, and combines the results into the output embedding."
      ],
      "metadata": {
        "id": "gvT5K8UuiYBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        # Ensure embedding dimension is divisible by number of heads\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads  # Dimension per head\n",
        "\n",
        "        # Linear layer to compute queries, keys, and values in one go\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape  # B: batch size, N: sequence length, D: embedding dim\n",
        "\n",
        "        # Compute queries, keys, values and reshape for multi-head attention\n",
        "        qkv = self.qkv(x)  # Shape: (B, N, 3*D)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)  # (B, N, 3, num_heads, head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)  # Rearrange to (3, B, num_heads, N, head_dim)\n",
        "        Q, K, V = qkv[0], qkv[1], qkv[2]  # Split into queries, keys, and values\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, num_heads, N, N)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = attn @ V  # (B, num_heads, N, head_dim)\n",
        "\n",
        "        # Concatenate heads and reshape back to (B, N, D)\n",
        "        out = out.transpose(1, 2).reshape(B, N, D)\n",
        "\n",
        "        # Final linear projection\n",
        "        out = self.proj(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "T7AbOOJFHGBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Transformer Encoder Layer, which applies layer normalization, multi-head self-attention, and a feed-forward MLP with residual connections to process input embeddings."
      ],
      "metadata": {
        "id": "W6H_43kKieto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, embed_dim, num_head, mlp_dim,drop_rate):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.attn = MultiHeadSelfAttention(embed_dim=embed_dim,\n",
        "                                      num_heads=num_head,\n",
        "                                      dropout=drop_rate)\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.norm1(x))\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "yLGh6rxfGKcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Vision Transformer (ViT) model that converts an image into patch embeddings, processes them through stacked transformer encoder layers, normalizes the output, and uses the class token for final classification."
      ],
      "metadata": {
        "id": "2vwuct1Fi06m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, drop_rate, stride = None):\n",
        "    super().__init__()\n",
        "    self.patch_embed = PatchEmbedding(img_size=img_size,\n",
        "                                      patch_size=patch_size,\n",
        "                                      in_channels=in_channels,\n",
        "                                      embed_dim=embed_dim,\n",
        "                                      stride = stride)\n",
        "    self.encoder = nn.Sequential(\n",
        "        *[TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)\n",
        "        for _ in range(depth)]\n",
        "    )\n",
        "\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.patch_embed(x)\n",
        "    x = self.encoder(x)\n",
        "    x = self.norm(x)\n",
        "    cls_token = x[:,0]\n",
        "    return self.head(cls_token)"
      ],
      "metadata": {
        "id": "g6NghVL2KXF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inistantiating our baseline model"
      ],
      "metadata": {
        "id": "PSqRMBTDjMWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(\n",
        "    IMAGE_SIZE, PATCH_SIZE, CHANNELS,NUM_CLASSES,\n",
        "    EMBED_DIM, DEPTH, NUM_HEAD, MLP_DIM, DROP_RATE\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "n6AvYPVjMeEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "mMsKQVkQNz7y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params = model.parameters(), lr = LEARNING_RATE)"
      ],
      "metadata": {
        "id": "XNH9HtM5Oamw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function trains a PyTorch model for one epoch, updating weights, computing loss, and returning the average loss and accuracy."
      ],
      "metadata": {
        "id": "cDerRhymnDnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion, scheduler=None):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)  # Move to GPU if available\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.size(0)  # Accumulate loss\n",
        "        _, pred = torch.max(out.data, 1)          # Get predictions\n",
        "        total_correct += (pred == target).sum().item()  # Count correct predictions\n",
        "\n",
        "    # Return average loss and accuracy\n",
        "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "pN5id-BfO6LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function evaluates a PyTorch model on a dataset, returning its accuracy without updating the model’s weights."
      ],
      "metadata": {
        "id": "qXVIm83anUiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
        "    correct = 0\n",
        "\n",
        "    with torch.inference_mode():  # Disable gradient computation for efficiency\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            out = model(data)\n",
        "            correct += (out.argmax(dim=1) == target).sum().item()  # Count correct predictions\n",
        "\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "1M4FMe5RQzo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function trains a model for multiple epochs, tracks training and test accuracies, and prints progress after each epoch."
      ],
      "metadata": {
        "id": "mvi8H8V5nh_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, criterion, epochs=15, scheduler=None):\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # Train for one epoch\n",
        "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, scheduler)\n",
        "\n",
        "        # Evaluate on test data\n",
        "        test_acc = evaluate(model, test_loader)\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Store accuracies\n",
        "        train_accuracies.append(train_acc)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch: {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    return train_accuracies, test_accuracies\n"
      ],
      "metadata": {
        "id": "6Ts-DV_9Reny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies, test_accuracies = train_model(model, optimizer, criterion)"
      ],
      "metadata": {
        "id": "qvS3JWtncqhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ploting the Epoch vs Accuracy for visualizing the trend"
      ],
      "metadata": {
        "id": "YaBufm13nu9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Accuracy_plot(train_accuracies, test_accuracies):\n",
        "  plt.figure(figsize = (10,7))\n",
        "  plt.plot(train_accuracies, label = 'Train_accuracies')\n",
        "  plt.plot(test_accuracies, label = 'Test_accuracies')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Trainig and Test Accuracy\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "S5nZYq7LSNSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy_plot(train_accuracies, test_accuracies)"
      ],
      "metadata": {
        "id": "M7F1ZoHocvpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating the total accuracy considering the complete test datatset"
      ],
      "metadata": {
        "id": "Xcgw2MN1n6sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def total_test_accuracy(model):\n",
        "    overall_test_accuracy = None\n",
        "    total_correct = 0\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    for data, image in test_loader:\n",
        "        data, image = data.to(device), image.to(device)  # Move to GPU if available\n",
        "        out = model(data)\n",
        "        _, predicted = torch.max(out.data, 1)  # Get predicted class\n",
        "        total_correct += (predicted == image).sum().item()  # Count correct predictions\n",
        "\n",
        "    # Compute overall accuracy as a percentage\n",
        "    overall_test_accuracy = total_correct / len(test_loader.dataset)\n",
        "    return overall_test_accuracy * 100\n"
      ],
      "metadata": {
        "id": "eZW7LOpiSSCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inital_accuracy = total_test_accuracy(model)\n",
        "inital_accuracy"
      ],
      "metadata": {
        "id": "GZPJJXbHXCF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "8AnKDpqwqsr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Let's apply some Data Augumentation technique"
      ],
      "metadata": {
        "id": "67spxDKkbeQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Chaning transform variable\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                     std=[0.2023, 0.1994, 0.2010]) ## actual calculated statistics for the dataset\n",
        "])"
      ],
      "metadata": {
        "id": "5p84fZ73XFvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset, train_loader, test_loader = load_data(transform_train)"
      ],
      "metadata": {
        "id": "UvWixy9Ddgd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Retraining the model\n",
        "model_2 = VisionTransformer(\n",
        "    IMAGE_SIZE, PATCH_SIZE, CHANNELS,NUM_CLASSES,\n",
        "    EMBED_DIM, DEPTH, NUM_HEAD, MLP_DIM, DROP_RATE\n",
        ").to(device)\n",
        "optimizer_2 = optim.Adam(params = model_2.parameters(), lr = LEARNING_RATE)"
      ],
      "metadata": {
        "id": "tl_B9orueuKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_model_2 = 15"
      ],
      "metadata": {
        "id": "dAuh5YL_l8oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies, test_accuracies = train_model(model_2, optimizer_2, criterion, epochs_model_2)"
      ],
      "metadata": {
        "id": "Xe8CuvZjfT50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy_plot(train_accuracies, test_accuracies)"
      ],
      "metadata": {
        "id": "Z5H7l0a9fZp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augemented_data_accuracy = total_test_accuracy(model_2)\n",
        "augemented_data_accuracy"
      ],
      "metadata": {
        "id": "6cfCqWQzivTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Let's use the AdamW optimizer with a scheduler and augmented data."
      ],
      "metadata": {
        "id": "N4h1p2kAjAox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_model_3 = 15"
      ],
      "metadata": {
        "id": "e9LsrCkimIMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = VisionTransformer(\n",
        "    IMAGE_SIZE, PATCH_SIZE, CHANNELS,NUM_CLASSES,\n",
        "    EMBED_DIM, DEPTH, NUM_HEAD, MLP_DIM, DROP_RATE\n",
        ").to(device)\n",
        "optimizer_3 = optim.AdamW(model_3.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer_3, T_max=epochs_model_3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Ltc207_ni-87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies, test_accuracies = train_model(model_3, optimizer_3, criterion, epochs_model_3, scheduler)"
      ],
      "metadata": {
        "id": "B0mogCmhkh4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy_plot(train_accuracies, test_accuracies)\n",
        "augumented_adamw_accuracy = total_test_accuracy(model_3)\n",
        "augumented_adamw_accuracy"
      ],
      "metadata": {
        "id": "_wLJEJz0kn0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Let's do analysis with Overlapping Patches. Here we are using Adam instead of AdamW because we saw that it was giving less accuracy for same number of epochs"
      ],
      "metadata": {
        "id": "xbtOaLkalA3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_model_4 = 20"
      ],
      "metadata": {
        "id": "R11QkDgPmWua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4 = VisionTransformer(\n",
        "    IMAGE_SIZE, PATCH_SIZE, CHANNELS,NUM_CLASSES,\n",
        "    EMBED_DIM, DEPTH, NUM_HEAD, MLP_DIM, DROP_RATE, stride = 2\n",
        ").to(device)\n",
        "optimizer_4 = optim.Adam(params = model_4.parameters(), lr = LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "JRQntcack8uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies, test_accuracies = train_model(model_4, optimizer_4, criterion, epochs_model_4)"
      ],
      "metadata": {
        "id": "-gLxdpVhScpq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy_plot(train_accuracies, test_accuracies)\n",
        "overlaped_patches_accuracy = total_test_accuracy(model_4)\n",
        "overlaped_patches_accuracy"
      ],
      "metadata": {
        "id": "QfVIk85hSjCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "0Fo3WJZbPMnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = {\n",
        "    \"Experiment\": [\n",
        "        \"Baseline - Non Overlapping, No data augumentation (15 epochs)\",\n",
        "        \"With Data Augmentation (15 epochs)\",\n",
        "        \"Augmentation + AdamW + Scheduler (15 epochs)\",\n",
        "        \"Augmentation + Overlapping Patches + Adam (20 epochs)\"\n",
        "    ],\n",
        "    \"Test Accuracy (%)\": [\n",
        "        65.48,\n",
        "        69.14,\n",
        "        69.28,\n",
        "        76.27\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "from tabulate import tabulate\n",
        "print(tabulate(df, headers='keys', tablefmt='pretty', showindex=False))\n"
      ],
      "metadata": {
        "id": "EFP5e-QQXXfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting random images from the dataset and predicted the label using our trained model"
      ],
      "metadata": {
        "id": "hpWjUVkNrPUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_plot_grid(model,\n",
        "                          dataset,\n",
        "                          classes,\n",
        "                          grid_size=3):\n",
        "  model.eval()\n",
        "  fig, axes = plt.subplots(grid_size, grid_size, figsize=(9,9))\n",
        "  for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "      idx = random.randint(0, len(dataset)-1)\n",
        "      img, true_label = dataset[idx]\n",
        "      input_tensor = img.unsqueeze(dim = 0).to(device)\n",
        "      with torch.inference_mode():\n",
        "        output = model(input_tensor)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "      img = img/2 + 0.5\n",
        "      npimg = img.cpu().numpy()\n",
        "      axes[i,j].imshow(np.transpose(npimg, (1,2,0)))\n",
        "      truth = classes[true_label] == classes[predicted.item()]\n",
        "      if truth:\n",
        "        color = 'g'\n",
        "      else:\n",
        "        color = 'r'\n",
        "\n",
        "      axes[i,j].set_title(f\"Truth : {classes[true_label]}\\n, Predicted: {classes[predicted.item()]}\", fontsize = 10, c = color)\n",
        "      axes[i,j].axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "gCDjYt8XPmFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_plot_grid(model_4, test_dataset, test_dataset.classes)"
      ],
      "metadata": {
        "id": "eUdgE8N8SMH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYZH6dW7kj25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}